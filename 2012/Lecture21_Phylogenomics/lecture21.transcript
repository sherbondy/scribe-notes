Audio Transcription – L16
All right, I’m glad to be here today. I’m a former student of Manolis, and my research was on phylogenetics, especially studying gene duplication and losses. So the lecture today is to finish up the phylogenetics lecture that you guys had on Tuesday, and also talk a little bit about more advanced uses of phylogenetics.
So there are four main things that I’ll talk about. I’ll finish up phylogenetics – particularly the Maximum Likelihood Method. I’ll talk about how we can use phylogenetics for orthologs and paralogs, and also detect gene duplication and losses, and why would we want to do that? Once we can do this, we can do that across a whole genome, and this kind of applying phylogenetics to the whole genome is this term phylogenomics. And then, lastly, I’ll talk about what happens when we do phylogenetics at shorter time scales. At shorter time scales, you have to think about an entire population of individuals, and that has some effects on what kinds of trees you expect. And so there’s a lot of research on this side of the field dealing with phylogenetics and populations.
All right, so, I’ll start with just finishing up with the phylogenetics part. So there’s a lot of different methods for building trees, and they kind of fit into two main workflows, or pipelines. I think Manolis talked on Tuesday a lot about this first pipeline – so distance-based methods. And just to review, the basic idea is that you have some sequences you’re interested in. They could be DNA or protein sequences, and you’d like to know their history – the tree that best describes the way they diverge from one another. And in this strategy, to get to this tree – that’s our goal – we go through a middle step of computing pair-wise distances. So you have a matrix of all the different pairs of these sequences, and what percentage of sites are different between them. I reviewed a couple of ways of estimating distances. So the number of differences between two sequences under-predicts exactly how much time has passed between them. That’s because you could have a site that mutates twice, or mutates to one letter and back. And so there’s different ways of correcting the observed distance and the actual distance. Jukes-Cantor and Kimura are two ways of doing that. And once you have this matrix, you can then reconstruct the tree, and there’s several clustering-like methods that do that. My understanding is that you covered UPGMA, and also another method called Neighbor-Joining. And they have different assumptions about the properties of the distances – whether they’re ultrametric or whether they’re additive. In practice, neighbor-joining is the more popular and better performing method.
But for this kind of pipeline, we might be concerned that this distance matrix over-summarizes your data. You throw out all the individual characters and you just represent a single-number distance between each pair. So there’s this second pipeline that we could do where we go directly from the characters to a tree. And so there’s lots of different methods that do this. On Tuesday, you probably covered the parsimony method way of constructing a tree. And my goal today is to kind of focus on this pipeline a little bit more. In particular, describe how Maximum Likelihood works.
So one thing about these distance-based methods is that there’s a fixed number of steps you do to perform the clustering. UPGMA and neighbor-joining can be done in cubic time. If you play with the definition of exactly how you do the clustering, you can get it down to quadratic time. Unfortunately, many of the methods in this type of pipeline are NP hard, so it’s known that parsimony is NP hard, and so is Maximum Likelihood. And so that means we have to resort to heuristics in order to find the phylogenetic tree. In practice, it’s not too bad.
The main heuristic that they use is this kind of workflow for finding a tree. The basic idea is that we want to search through different trees and test out each one. And so we want to start an initial tree, and we can use the neighbor-joining algorithm to get an initial tree. And then we want to compute some score, and the score depends one exactly what method you’re doing. It could be some kind of parsimony cost. It could be Maximum Likelihood, it could be a likelihood, it could be a posterior probability. And we save that score off to the side. Then, we want to explore the tree space.
So there’s several methods that take a tree and slightly modify it. Nearest-neighbor interchange is one of them, sub-tree pruning and re-graphing. You modify it a little bit, compute your score again, and you loop and search the tree space until you think you’ve searched enough. And then at the end, whatever tree attains the lowest cost with the highest likelihood, that’s the tree that you output as the answer.
So let’s look at ways of proposing new types of trees. So, a very simple and popular one is called nearest-neighbor interchange. And the idea is that you have your tree, and you pick some kind of internal branch within the tree. You could pick it randomly, or have a certain order in which you visit the internal branches. And for every internal branch, there are four sub-trees that are going to come off that branch – A, B, C, D. And if we swap sub-tree A with either C or D, we’ll get a new kind of topology. And what’s interesting about the nearest-neighbor interchange operation, is that starting from any tree, we can go to any other tree in the tree-space, just using nearest-neighbor interchanges. And so that’s a useful property to have for any kind of operation, because you wouldn’t want a certain part of the tree-space to be impossible to explore. Unfortunately, though, the tree-space is very big, and so this is part of the challenge with making these algorithms efficient when they’re NP hard. The number of un-rooted topologies is almost a factorial. So it’s factorial, except you skip just the odd numbers, where n is the number of leaves in your tree. And rooted topologies is even more – basically, it’s the number of un-rooted topologies, and then all the different ways you can root an un-rooted tree into a rooted one. And so just to give you an idea of how fast it grows, there’s only one un-rooted tree of 3 leaves. Of 4 leaves, there’s 3 different types of trees you can do. If these sub-trees, for example, were just a single node, these are the 3 types of trees of 4 leaves. For 10 leaves, it goes up to 2 million. For 64 leaves, you have more than a google number of topologies. And so we can only explore a subset of that space.
So now we can search through the space, score trees, and there’s different ways that we can let the score guide our search. So try and go towards parts of the tree-space that seem to have better scores for lower cost or higher likelihood.
So let’s talk about how this probability can be computed. In particular, I’ll talk about the Maximum Likelihood algorithm. And in maximum likelihood, a likelihood is the probability of our data. In this case, it’s our n gene sequences – x1 through xn – conditioned on our model. And our model, in this case, is some kind of tree topology T, and a vector of branch lengths, t. And in maximum likelihood, we want to find the tree topology and branch lengths that maximizes this term. So in order to do that, the actual maximizing over topologies, that is done by this search procedure I described. The branch lengths can be maximized numerically. We’re not going to get into that too much today. But what I want to describe is exactly how can we define this term, and define it in a way that it’s efficient to compute for each proposed there that we’re going to see in our search. And so, to find that term can be done in three steps. First, we could think about how we could find how a single site evolves along a single branch. Once we have a model for that, we could build on it to make how a sequence could evolve along a single branch, and then build on that to find how a sequence evolves along an entire tree. And then once we define was this term is, we need to think about whether we can compute it efficiently. And the answer is that we can, and that there’s a special dynamic programming algorithm for doing that. And so those are the things that I want to talk about.
So let’s look at site evolution. So on Tuesday, you were introduced to the Jukes-Cantor model. So this is one of the simplest models that you could define for how DNA bases evolve. Jukes-Cantor is a continuous time Markov chain. And this is like the Markov chains you’ve seen in class, except instead of transitioning between states in discrete points in time, think of it as transitioning in continuous time. So you can transition at any point. And so, instead of defining probabilities of transitions, you need to find rates – instantaneous rates of transition between all 4 bases. And Jukes-Cantor is the simplest model, where transitioning between all the bases is equal – some rate alpha. Now, in our case, we’d like to have a model of how a single letter, or base, mutates from the top of a branch over some length time t, to some other base, say C, at the bottom. And so, in particular, we want to know this probability – that given we start with A, after some time t, what’s the probability that we mutate to C? And this probability can be found by defining a discrete version of this Markov chain. Basically, the probability of transitioning over a given period of time. And so, there’s a way of deriving basically a probability transition matrix – probability of going from any of the 4 letters, transitioning to any of the other 4 letters. The Durbin book will derive how you can get this matrix from this definition.
But let’s just think about what this means intuitively. So I have two types of entries here: I have the diagonal, which is rt, and it’s this expression, 3e^-4αt. It’s a little fuzzy, I apologize. And then all of the off-diagonals are x. And so what would this matrix look like if we plugged in a time of 0? Think about what that matrix would look like. The diagonals would be 1, and then the off-diagonals would be 0. Okay, so, this would look like the identity matrix. And that makes sense – that if no time passes, t = 0, you have no opportunity to mutate. Your ending base is exactly the same as your starting base. So what happens as t goes towards infinity? All 1/4ths. And that makes sense, too. So as time goes on, the probability of being any base becomes ¼. And for t in between those two extremes, it’s kind of a blend between the two. So once your probability mass is towards whatever your starting base is, and as time goes on, it diffuses towards the off-diagonals. And, depending on your model, you can have the long-range base frequency be something different. ¼ for each base is a simple version, you can have unequal base frequencies, things like that.
So that defines how a single site could evolve. How could we decide how a sequence evolves? So our problem now is that we have some sequence xk at the top of our branch, and we have some time ti, and we want to get to the sequence xi. So this probability. So here’s the probability of the sequence evolving from xk to xi. And we could just factor that into a probability of each of the sites, j, evolving from some base A to B. So that’s a common assumption that you see probably in a lot of algorithms. Just to test it, though, can you think of any cases where this might be violated in real life? So you have bases that, on different parts of the sequence that are supposed to fold and bind, and so if you mutate one site without mutating the other, you might break that binding. Can you think of another example? What about genes, at the nucleotide level? So codon translation would be a simple example of that. So, every triplet of bases translates to a particular residue, and so when you change one of those bases in the triplet, it might change the probability of the other bases substituting. But you can either try to only work with sites that are independent – maybe just work with the silent site. Or assume that this is a small effect.
So, now we know how a sequence evolves along a branch. Can we define how it goes across an entire tree? So, to do this, we can add one more assumption. And that is to assume that branches evolve independently once we condition what the parental sequence is. So if we have n sequences at the leaves, we have 2n-1 sequences over the whole tree. And so the probability of all these sequences in the tree, we could factor into just the branches. So the probability of every sequence, given whatever its parent sequence is, over time ti. And the only node that doesn’t have a parent is the root. So we have to find some kind of prior distribution for what the sequence at the root is. And there are many ways to define that. One simple one could just be to assume that the sites are independent and that a site could have any base with ¼ probability, so ¼ raised to the power of n, when n is the length. We can use this product to find the probability of the entire tree.
So, can we compute this term efficiently? Alright, so, one thing you might’ve noticed is that we haven’t quite defined what we wanted. We’ve defined the probability distribution of all the sequences, but this goes only up to 2n-1. So this is both internal and leaf nodes. But in practice, we’re only going to get DNA sequence from modern genes, so only sequences at the leaves. So if we don’t know the sequences in the internal nodes, we have to marginalize, or sum over the possibilities. So we can compute the probability we want, which is the probably of sequences x1 up to xn, from the one that we’ve defined, which is all the sequences. We just marginalize over xn+1 all the way up to x2n-1. So a bunch of nested summations here. That, at first glance, looks very expensive, doing all those nested summations. But we don’t have to worry because we can use dynamic programming to do it efficiently.
So there’s a basic trick to how you can do that. And we can get an idea about it by just looking at a very simple example. So say I had two summations i and j, and they go between 1 and n. Let’s say we have two functions of i and j. How much time do you think it would take if you compute this, just naively, if you compute it exactly the way I wrote it? N^2. Can you think of a way that we could rearrange this equation so that it’s a little bit more efficient. So, you notice that f(i) doesn’t depend on j. It’s constant with respect to j, so we could factor it outside of the j summation. So, naively looks like n^2, so we start with this, and we notice that f(i) doesn’t depend on j and we can move it outside of the j summation. Now when we do that, we notice that he j summation – that entire expression – doesn’t contain i. So it’s constant with respect to i and we can move that out. So now we have the two summations side by side. So how long does it take to compute that?
So the basic idea with the tree is we could just apply that for every internal node in the tree. So here again we have the likelihood that we want to compute. We have the full joints of all the sequences. We’re going to sum over the possibilities for all the internal nodes, so x5, x6, x7. I’ve shown how we could define this as a probability of multiplying all the branches evolving. So I’m just including every branch and its parent, and then a prior on the root sequence. And now we can apply the trick. We’ll notice that there are three terms that contain x5, but they don’t contain x6. So we can move the x5 summation just to those three terms, and move the x6 summation out of there. And we apply the same idea to three terms that contain x6, but not x5. And so now you notice that there’s only two summations nested instead of three. And the idea is that you can keep applying this to every sub-tree. Move the summations for two sister sub-trees apart from each other. And so you can define that dynamic programming table that basically applies that same idea.
I think it’ll be most helpful to look at this slide afterward, but I’ll just say a couple of ideas of how it applies the trick. Basically, you fill in this table Li,j,a. So it’s three-dimensional. i ranges over over all of your nodes, j ranges over all of your sites, and a ranges over the 4 possibilities of the nucleotide at that position. And you fill in the table, starting with the leaves. So basically, you put a 1 in the table for a nucleotide A, if that’s actually what you see in your leaf sequence. i<n means that this is a leaf case. And if it’s not that case, then it’s a 0. And then you populate this table in post-order traversal, so going up the tree. And this summation here basically is applying our trick that I showed you in the previous slide. It’s just written in a notation that’s very general so it applies to any tree.
So in terms of the amount of work in running this algorithm, we just need to fill in all the entries in the table. So there are n nodes, m sites, and there are let’s say, if our alphabet in our sequence is a size k (so for DNA that’s 4, for peptides that would be 20), we would just need to do k^2 amount of work to fill in that tree. So the runtime just scales with our input data slides. So that’s pretty nice.
So that basically concludes how Maximum Likelihood works. To think about how parsimony would work, it’s a very similar concept. Only this term, instead of being about the probability of one base turning into another, it’s just a count of whether it changes or not. So it’s like a 1 or a 0. It’s a weighted parsimony. So instead of probabilities, there are costs. And instead of summing over them, we would do a min – the minimum cost substitution.
So this is just an overview of where Maximum Likelihood fits into the literature of phylogenetic methods. The oldest method is UPGMA. Least squared error is another distance-type method, but it actually also has this kind of search procedure. Maximum parsimony came next, then Maximum Likelihood. Neighbor-joining is actually more recent, and it’s distance-based. In the 90s, Bayesian methods became popular. There’s a way of modifying this Maximum Likelihood to make it find the maximum posterior tree, basically a prior on what kind of tree topology and branch lengths you expect beforehand. And kind of the latest work which I’m going to talk on now is basically ways of applying phylogenetics and making improvements to these methods that are custom-tailored to your exact questions. So if you’re studying gene duplications and losses, or population effects. And there’s lots of different software for implementing all these methods.
So that’s what I’m going to talk about next, is one of those applications, which is using phylogenetics to infer orthologs and paralogs. Alright, so, there’s actually two kinds of trees that are commonly used, especially in studying duplications and losses. One of them is called the species tree. And that’s a tree where the leaves are labeled with species, and the tree tells you how these species diverge from one another. Now, this might be the tree that seems most obvious to you right away. There’s lots of ways of obtaining species trees, mainly using these phylogenetic methods. So we need some kind of core or single-copy gene that is representative of the species. Genes that satisfy this behavior are ones that are single-copy, if they’re very conserved, if their mutation rate follows a clock, that is very nice. There’s some complexities in doing this problem. So there’s lots of methods to build on the phylogenetic algorithms to handle that. I’m going to talk about that in the second half of the lecture, mainly with coalescence. But for now, let’s assume we have a black box that builds you a species tree.
So the second kind of tree that we’re interested in studying is called a gene tree. It’s a tree where the leaves are labeled with actual gene sequences. So let’s say these are gene ids. And each gene has some kind of sequence that can be nucleotide or peptide. And in the simplest case, these two trees are exactly the same. You have some gene that’s in one copy, let’s say in Human. And it’s also one copy in all the other species. And that is because their common ancestor had one copy, and its just been inherited down the tree very simply. So you can think of the gene tree as evolving inside of the species tree.
Now there’s lots of events that could happen along the way. I just want to talk about two of them right now. One of them that could happen is that, in the course of DNA replication, a gene gets deleted. So it was present in the past, but now it is lost from the genome. So it will be missing in any species that descend from that point in the tree. And another thing that can happen is that a gene can duplicate. And there’s lots of different events that can cause gene duplication. Unequal crossing over, a lot of DNA repair mechanisms accidently duplicate the DNA around the repair site, genome rearrangements like inversion often accidentally duplicate DNA, retro-transposition can cause it too. And so, in this model, you would have one gene that’s suddenly two genes, and then it’s two copies from that point on. And so it has this unusual effect of causing the gene tree to have two sub-trees that are copies of each other. So we have this mouse, rat tree that appears twice. And of course these events can happen multiple times in the tree, even in the same lineage. And so your gene tree can look arbitrarily different from the species tree.
There’s probably some terms you’ve been using throughout the class, orthologs and paralogs. And we can now define these rigorously using a phylogenetic tree. So two genes are considered orthologs if their most recent common ancestor is a speciation, or this bright circle. And they’re considered paralogs if their most recent common ancestor is a duplication. So just to test your understanding, what is the relationship between d1 and m1? Ortholog, right? Because their most recent common ancestor, if you trace it up, is that one. Alright, so what is the relationship between m1 and m2? Paralog. What about the relationship between m2 and d1? It’s also ortholog. So it doesn’t matter which genus maybe the older one or newer one – they both have an orthologic relationship to other species. There’s some unusual cases like m1 and r2 are paralogs.
Alright, so what we would like to do in our research, is we wanted to make basically this kind of tree for all the gene families in the genome. For, let’s say, dozens of species. So that would be about tens of thousands of these kinds of pictures. And so we wanted to be able to automate that process. So how could we define an algorithm to do that? Well, we can use phylogenetic algorithms to get pretty far. So we could use some kind of phylogenetic algorithm to get our species tree. And then we could use the actual gene sequences of a gene family to build our gene tree. You can use whatever method you like. But then we need to figure out how this gene tree fits inside the species tree. And that’s another algorithm called reconciliation. It figures out, basically, a mapping of nodes inside the gene tree to the species tree.
And so there’s many different ways you could map the gene tree to the species tree, so we need some kind of metric for saying which one is the best. So we could use parsimony as a principle for defining the best. And each of these mappings infers a different number of duplications and losses. So, given a gene tree and species tree, our goal could be to find the mapping, r, that implies the fewest duplications. r, some kind of mapping that maps the vertices from the gene tree to vertices or edges on the species tree. So this kind of idea was first proposed in the late 70s. In the 90s, it was formulated more rigorously as a computation problem. And an algorithm was given for it – it was quadratic. But it’s shown that you can do this in linear time, and you can change the optimization. You can say, let’s find the mapping that has the fewest losses. Or we can define it in a probabilistic way, into a Bayesian reconciliation.
So let’s just get an intuition about how these mappings work. I’m showing you one particular mapping. Now, can you think of another way we could map these two trees? The labels are fixed, you don’t get to change the trees – just the mapping. So the duplication could have happened first (*drawing on the board*), it goes down here, and then off this branch, we would have one copy of mouse and rat. And this copy, also, has mouse and rat. So these are speciations. We don’t see a contribution to dog here, so that must have been a loss. And we don’t see a contribution to human, and this is a loss too. So are there any mappings that don’t make sense? That are nonsensical? Maybe it’s too obvious. So, basically, what you proposed was that this duplication could map higher in the tree, right? So what I was looking for is that we can’t have any child mapped to a node that’s older than its parent, unless there’s time travel. So there’s many mappings that – you know – it’s technically a mapping, but it doesn’t make sense in terms of biology.
So in terms of finding a parsimonious reconciliation, what do you think a general good strategy is? So worse ones tend to be when you consider an event happening older than it clearly can. So if you can kind of push events as close to the recent as you can, you’ll prevent yourself from inferring extra losses. So if you make a duplication older, that just means you have to infer lots of losses. Alright, so, you can define a recursive algorithm to do this. Mapping r of some node, v, in the species tree. For the leaves, we were told what species they came from. We know we’re looking at a human gene or a dog gene. So we can define – I mean, there’s technically arrows at all these leaves, I didn’t draw them because it gets messy – but you can initialize the mapping for those. And then we can go through post-order traversal, so up the tree, and the mapping for any node, v, is just the least common ancestor – also called the most recent common ancestor – of whatever its right child maps to, and whatever its left child maps to. So you can test this – say, m1 maps to mouse, r1 maps to rat, so we have these two nodes. Their least common ancestor is a rodent. And we can see that is what this reconciles to. So where does this duplication map? Well, both these nodes map to the rodent. The LCA of two rodents is rodent, and so it maps to the same node. And then if we want to count the number of events, it turns out that v is a duplication, we should put a star on it, if it maps to the same node as either of its children. And we know that there’s losses basically if there’s a gap in the mapping. So if our parent does not map to the same node, or to the parent in the species tree. So this node doesn’t have a mapping, so there must’ve been a loss in here.
So now that we have these algorithms, we could systematically create these gene families and systematically count duplication and loss events and know where they happened. We could do a lot of analysis about which gene families seem to be expanding. That’s interesting because creating gene duplications is a major way of creating new types of genes and functions. One thing about it is that most mutations are deleterious. But if you have a backup copy of the gene, as long as the mutation isn’t dominant negative, it doesn’t somehow interfere with your original copy, it’ll be allowed. And so this can be a way of exploring a sequence base more easily. And maybe the gene stumbles upon a new function, or it acquires mutations until its lost from the genome. So a lot of gene families, like olfactory receptors, are highly duplicated – basically you duplicate a receptor, maybe you modify it a little bit to detect a different molecule. Also, tricolor vision in primates arose this way. So most mammals can only see two colors cause they have only two opsin receptors. And one of those duplicated in the old-world monkeys, and customized itself a little bit through mutation to detect a distinct color. And so you can use gene tree building to reconstruct that event.
So we could explore those questions systematically for a lot of gene families if we had some kind of computation pipeline. And so a lot of the algorithms you’ve seen in class can be strung together. If you have all the genes from all the species you’re interested as your inputs, and also if you have a species tree beforehand, we could use BLAST to compare all the sequences together. We could use some kind of clustering method to put them into putative gene families. We could use alignment algorithms to make a multiple alignment. From that, we could use any phylogenetic method to make a gene tree. And then we could use a reconciliation algorithm to compare to the species tree, and once we do that we can then infer orthologs, paralogs, duplications, and losses.
And so there’s lots of different databases that do this. We have our own, SPIMAP, and they all have different advantages. I just want to briefly talk about one insight we had in our own method. It had to do with how errors propagate in this pipeline. So you could imagine that if you make an error – say at the tree-building step – this might screw up how you do reconciliation, and screw up what kinds of duplications and losses you infer.
So let’s look at what an error might look like. Let’s say we have this gene tree and species tree, and let’s say this is the correct answer. Somehow we know that. The two trees are congruent, so we infer there are no duplications and losses. Now let’s say our phylogenetic algorithm made a mistake, and accidently proved that human branched out of dog instead of mouse and rat. If you run the NPR algorithm in your head, what kind of reconciliation would we get? So this node maps to the same place. Can you figure out where this node maps to? Yeah, it maps to the root of the species tree. Because it’s two children mapped to dog and human. And the LCA of dog and human is the root of the species tree. So if you draw out that mapping and then you apply the rules for duplication and loss, you’ll be forced to infer one duplication and at least three losses. And this is just because of a single branch being misplaced. So if this happens frequently, you can dramatically over-estimate these types of events, and it will screw up any kind of downstream analysis you want to do, inferring the rate of them and what families are being duplicated and things like that.
So how frequently could these occur? We did a simulation study using different types of phylogenetic methods, these three colors here. How many duplications did we infer? What was our precision? You could over-estimate the number of events two- to three-fold. And it gets tougher for larger species trees in your simulation. This blue bar, our method, doesn’t degrade as poorly as you go to tougher trees and things like that.
So I just want to give a little bit of an idea of what you could do. This is an idea of what kind of ideas are currently in the field. So phylogenetic methods – the traditional ones I’ve explained – typically ignore th species tree. If you’re building a gene family, it just works with the genes. And it just looks at the sequences. There’s no concept of where it looks at the gene in, it doesn’t know if it’s looking at a human or mouse gene. And so our insight is that, if you add as an input the species tree, you could use that to find a reasonable prior for what kinds of trees you would expect beforehand. And to get an idea of what that looks like, say we had two different proposals for these 4 genes. One that’s congruent with the tree, and one that’s not. When you reconcile it, you see that there’s one gene branch in each species tree branch. But when you reconcile an incongruent tree, it’s forced to take a very convoluted route to the species tree. In particular, this human branch now has to go through a longer period in time, because it has to reconcile all the way up to the root of the tree. So if we knew the species tree, we could know beforehand that we expect the human branch to be longer. So it is more unlikely to have these events, but if the sequence supports a longer branch, then maybe that’s okay. Maybe we should accept the alternative tree. But if it doesn’t, and I’m trying to stretch this branch longer than it should be, we’ll get a low probability.
So how could we develop a model for what kind of branch lengths we expect. A probability distribution over branch lengths? We did this by doing a case study on real sequence data. So this is part of a project of studying 12 fly genomes. And about 1/3 of their genomes. They’re about 60 million years apart. That’s how deep the tree is. So that means that about 1/3 of their genome has this kind of conserved gene order. The other 2/3 is very rearranged – the genes appear in different orders. But one thing you can do is use this conserved gene order to tell you that these are very likely orthologs, without having to build a gene tree. And so we could build a tree for each of these – these boxes represent trees. So if we do that, we find that they have the same topology, but the branch lengths differ quite a bit. But one thing you might notice is that they kind of look like scaled versions of each other. So here, if we normalize, you can see it’s kind of roughly the same types of branch lengths. There’s still some slight differences. So how could we develop a model for that?
Another way of quantifying that effect is we could look at these absolute branch lengths and do a scatter-plot of all 5,000 of these families that we have. And you notice that when one branch is long, so is the other. So there’s correlation. And that’s true for all pairs of branches in the tree. So when a gene is fast-evolving in one species, it tends to be fast-evolving in all species. That gives you this kind of scaling effect. And then if you normalize the branch lengths, you find that this correlation goes away and it goes away for nearly all of the branches.
So this is convenient in terms of modeling. It means that we could model rates of substitution as two different components. So one component we call a gene-specific rate, that basically defines a rate that’s present across the entire gene family, whether this is going to be a fast or slow family. And then we have species-specific rates that customize how that individual branch is. And just to give you a high-level picture of what the algorithm does, we can now look at the actual branch lengths here and compare them to the distribution based on this model. And if the branch lengths are near the middle of these distributions, we might say this looks like a very likely tree. But, for example, this human branch, which we expect very long distribution for, but it’s a little bit too short. And so is this branch for the same reason. And the dog’s is a little bit too long. And so we would get a lower probability, because the branch lengths seem more unusual from what we expect. And the method performs well, but we don’t need to talk about that.
That concludes the ideas that are going on right now with phylogenetics and gene duplication and loss. And the last topic I want to talk about is what happens when we look at shorter time scales? And the main thing there is that we have to be very conscious of the fact that there’s a population of genomes evolving. So the reason why this is becoming more and more of a big issue is that people keep sequencing genomes. So when I started, I had like 4 mammalian genomes to work with – human, mouse, rat, and dog. Now there’s about 30 mammalian genomes. And there’s even a project to try and sequence eventually 10,000 vertebrate genomes in anticipation that sequencing costs just keep going down. Right now, people have put up money for sequencing about 100 of these. But DNA samples are being collected right now for all of these different species. So as we add more species to the species tree,  we’re going to be breaking up these branches into smaller and smaller branches, so we’re going to be dealing with trees that happen in smaller units of time. And so what can happen in small units of time?
To understand that, there’s two models I want to talk about that help model what could happen. One of them is called a Wright-Fischer model. It’s a very classic model, used to study all kinds of things, like drift, selection, population bottlenecks. And then another model that combines the Wright-Fischer model with trees. And that’ll be very useful for phylogenetics in these types of situations.
So I’ll describe to you the Wright-Fischer model. Basically, the model was designed in order to study the effects of finite population sizes. So there’s a lot of theory about how allele frequencies should vary in the population in very ideal cases – say, infinite population size. The Hardy-Weinberg equilibrium tells you what to expect in that case. But allele frequencies can very not because of selection or anything but just because of random chance. But there’s a finite number of individuals in populations. So that’s why this model was designed, and there’s lots of different uses for it – species migration, population bottlenecks, things like that.
So it has several assumptions. Among them is that you have a fixed population size, n, over time. So if we have n individuals here – lets assume we’re working with a diploid species, and let’s only focus on one chromosome in that species – and so each individual has two homologues of that chromosome. And so we have 2n chromosomes in the population. Some other assumptions are that these individuals randomly mate, the locus that we’re looking at is neutral, and that generations are non-overlapping. And even if these assumptions aren’t always held, this provides a good null model for when you want to test whether one of these assumptions is being violated, like selection is going on, or population structure has non-random mating.
So, the way that this model is defined is the next generation also has 2n individuals, and each chromosome chooses its parent from the previous population uniformly. And you can think of several biological stories for what might fit that kind of process. If each individual has many gametes, and then you randomly pair gametes to make new individuals, things like that. But the process I just described is the easier one to think about mathematically. So each chromosome picks uniformly one of the chromosomes from the previous generation. We can repeat that process as many generations as we want, and build up an entire population.
For our purposes, we’ll be studying just the DNA and it’s going to be very tough to collect information about actual ancestral individuals. So we can just remove the concept of individuals from the model, and just focus on what the chromosomes are doing. Now that we’ve taken that out, the left-to-right ordering of these chromosomes really doesn’t matter much. We can rearrange them to make the picture visually easier to look at. And when we do that, we see that these chromosomes are actually related to one another by many trees. And so you can think about a lot of different things with this model. One thing is to think about how new mutations propagate in a population like this. What’s the chance that it does out? What’s the chance that it spreads? The one that I want to talk about is to think about tracking how lineages go back in time.
So if we were to sequence, say, 4 people, from the modern population, how would we expect them to be related if this is the way their population was evolving? Well, we can track back in this population structure to find the tree. Well, what kind of branch lengths would we expect in this tree? How far back to we have to go before we find the common ancestor of a particular individual? And that kind of question and others like it can be addressed using the coalescent model. The coalescent model, what’s handy about that, is it’s only concerned about the handful of lineages that we actually have sequence data for. And we don’t have to think about all the individuals present in the population that we don’t have data for.
So the way we can think about that is, it is a probabilistic model that works backwards in time. So think about having 4 individuals here, and we go back in time until we find the first point in time that two of them have a common ancestor. And that kind of backwards view is called coalescence. And so we can go back further in time, find another coalescence. If we go back far enough, we’ll find a common ancestor of everybody.
And so what we’d like to know is what kind of probability distribution will we have for these time durations? The coalescent model is about approximations you can make when the population size is very large, and the number of lineages you’re tracking is very small in comparison.
So let’s think about how you could develop that model. Based off of the Wright-Fischer process, a lower level process. So let’s just look at two generations. Say we have 2n chromosomes, and we have k individuals we’re tracking. And we want to know what’s the probability that these k individuals do not coalesce? Or another way of saying it, is that they pick distinct parents. What would that probability be? Think about them choosing their parents one at a time. The kth guy can pick among the 2n, but he can’t pick whatever the k-1 picked. So the first person can pick whoever he wants, there’s no violation there. We just go like this, and represent it as a product. And if you work that out, the most important terms are the term that has 1/n in it, and then all the other terms will have n^2, n^3, and n^4… So this notation is saying that all the other terms are smaller than 1/n^2. And so for very large n, we can discard this term. And that turns out to be pretty handy.
So let’s just think about multiple generations now. What’s the probability that the first coalescence happens at the t generation? And this is a typo – j is supposed to be t. It’s like flipping a coin. So the fact that they don’t coalesce in this generation means that it’s this tree here. And now it’s the same situation. The probability that they don’t coalesce in this generation is the same. So it’s like getting tails a lot on your coin. And then the last generation we have coalescence, so it’s like 1 minus the probability that they don’t coalesce. So that’s what we have. We have, again this j should be t – there’s t-1 situations where they don’t coalesce. Finally they do coalesce. That’s a geometric distribution. And for very large population sizes, t will also be large. And so geometric approaches exponential for really low probability for your coin. The other assumption here is that for really large n, the chance that multiple lineages coalesce in exactly the same generation is pretty small. So let’s just think about just one generation coalescing. And now since t is very large, population sizes can be like 10,000 individuals or a million individuals, depending on the species. So t is going to be 10,000 or a million, and representing it as a continuous variable is not that big of a violation.
So you could write a simulator now that simulates these types of trees. Here’s some examples of that. Here’s a population size of 1,000, k =100. One thing you’ll notice is that, a lot of the time, nearly half the time is spent just waiting for the last two individuals to coalesce. And so, as you sequence more individuals in a population, you don’t really learn that much about what’s going on ancestrally. They’ll just coalesce really quickly to someone more recent. So that’s kind of a limitation to how much data you can really collect about a population from sequencing modern individuals.
So, what does this have to do with gene trees and species trees? So now what I showed you in the previous work, there’s this assumption that there’s one reference genome per species. We’re just thinking of this one reference genome evolving over time. But that’s not really true. There’s actually a population of genomes, and they can all vary from each other and each have their own history. And so we can think of the species tree – as actually, each branch has its own Wright-Fischer process inside of it. So there’s actually a population of 2n chromosomes that we’re tracking. And so when we sequence individuals and make a reference genome for species A, and B and C, and we compare them, the history that we’re learning about is actually a trace-back in this combined Wright-Fischer process of the whole tree. And so there’s a model that models this kind of situation, and it’s called the multi-species coalescent.
One thing to notice is that, in these leaf branches, it’s kind of boring. You’re just tracking one lineage, and there’s nothing to do. But eventually it gets up to here, and the chances are these two lineages will have distinct individuals as their ancestors. And so we have to wait an additional amount of time for them to find coalescence. So that means that there’s actually a lag time between when the population separated – let’s say, one migrated to the other side of a mountain range and they’re no longer mating. And then they start speciating. There’s a lag time between that separation of populations and when two actual gene lineages find a common ancestor. Now, if we’re working with fairly long branch lengths, this lag time will be pretty negligible. So assuming that kind of the gene tree speciates the same time as the species tree is a fine assumption. But if this branch length is pretty short in the number of generations, or if its pretty fat – if it has a lot of individuals – then something weird can happen. So just to point out, the rate of coalescence slows down if n gets bigger. So for short branches, and fat branches, it could be that these two lineages don’t coalesce at all. So now we actually have three lineages up here, and that’s called deep coalescence. Is there anything weird about this gene tree? Besides the deep coalescence. Something else. So you notice that C and D are sisters in the gene tree and they’re not sisters in the species tree. So the gene tree is incongruent with the species tree. And this is possible without invoking any duplication and loss. They’re all orthologs, they never duplicated. Just because of this population effect, we get incongruence.
So let’s say we’re tracking these lineages up and we find that there’s 3 individuals here at this population. We haven’t seen the top of the tree yet. What’s the probability that the tree will be incongruent? So there’s three possibilities, and it turns out that all three are equally likely, because at this point, the individuals don’t have any favoritism. You can think of a Wright-Fischer process, which just picks parents uniformly from the previous generation. It doesn’t make use of any information about where they came from or anything like that. So if it gets to this point, 2/3 of the time, you’ll have incongruence. Now, in practice, many of the coalescences will happen here, so you’ll still get congruence.
So this effective incongruence due to population effect is called incomplete lineage sorting. So this is a pretty interesting effect that people are studying right now. In 2006, there was a study of these 4 fly genomes. So this is Dmel, Dere, Dyak, and Dana. I think this goes back, I want to say, 4 or 6 million years. So that’s the time scale of the population size of these individuals. I think it’s thought to be around a million or so. That’s effective population size, so it’s a little bit lower than the actual number. What they did was they took a bunch of gene families and they built trees. And they found that they actually encountered three different types of topologies. One of them – Tree 1 – matches the species tree. But many of them, like Tree 2, actually have this incomplete lineage sorting. The blue and green did not coalesce right away, they coalesced deeper. And here’s a pie chart of how often these trees were found to occur. And you can see that incomplete lineage sorting for these species happens quite frequently. You can see that Tree 2 and 3 happen with equal frequency. So once you’re going to be incongruent, the different ways you can be incongruent are equal. It’s kind of like what we were talking about with the red – the fact that red or blue coalesce with black. That’s equal.
*In response to an inaudible question: So, in other studies, people like to study very recent speciations. I know of one study studying a recent speciation of some bird species in Australia. And there’s two populations – they’re separated geographically by quite a bit. And they have different markings. And they wanted to know whether they were really distinct species or whether they’re just different breeds of the same species. And so they did DNA sequencing to try and figure that out. And the idea is that you would expect the individuals of the same population to be closer to each other than to others. So that’s like sequencing lots of different individuals here, tracing them back. But early attempts at this type of problem did not quite handle the case that there might be individuals in population A that are closer to B than to other individuals in A, just because it’s very recent. So modern techniques kind of know about this and want to say enough separation – not perfect, but enough. And you can represent that probabilistically. So there’s applications where you would want to sequence multiple individuals and look at their coalescence.
So another interesting example is that this happens in the human genome. So this was a paper in 2007. They looked at 3 species – human, chimp, gorilla. And they used orangutan as the out-group. And the thing is, when you have a chromosome that has recombination, each part o the chromosome actually has its own trace-back throughout the lineage of the species tree. So the topology of the phylogenetic tree in particular actually changes across the genome. And the points at which it changes represents a recombination event that happened sometime in the population. And so with the human genome, there’s actually parts of the human genome, where there’s strong evidence that human and chimp are the most recent common ancestors, and gorilla out groups. But there’s large parts of the human genome where actually gorilla is closer to human, or chimp is closer to gorilla. These alternative topologies. And so this paper was about creating an HMM model to figure out what parts of the genome support one topology or the other. And then you could see how often these different topologies are supported. And the idea there – the reason they used an HMM model – is that sites that are next to each other are likely to have the same kind of tree. Sites that are far away from each other likely don’t. HMM is a nice way of letting sites influence each other to have the same tree. So what they did was they trained this HMM. The states are the different trees, and that’s kind of interesting. And now they can parse a genome alignment. So here’s a part of a genome alignment from chromosome X. And these curves are the probability of each state. So you learned posterior decoding in an HMM. So here is like posterior probability of each topology at that given location along the genome. And you can see that, for long stretches, I think red is congruent to Tree 1. But then there’s regions where the alternative topologies occur. And what’s kind of very interesting is that half the human genome deep coalesces. It coalesces with chimpanzee after the gorilla speciation. And what that means is that, 50% of the time you’re in HC1 or HC2.
So using this method they could find out how much incongruence would go on. Also, the percentage of incongruence gives you some information about divergence times between these branches. Basically, if you see a lot of incongruence, that means that a branch is fairly short. This is kind of an interesting way of determining divergence trends – not using nucleotide substitutions where you might have rate variation, but just incongruence. And you could also use population sizes, which is a useful statistic.
Just to wrap up, there’s a lot of applications of incomplete lineage sorting. You can use it to build species trees. One thing you might have noticed is if you try to build a species tree on one gene, that one gene might be incongruent. And you might infer the wrong species tree if you just trust one gene. So you should look at the gene trees of a lot of genes and have them kind of vote. And early work was just literally voting like what topology shows up the most. But there are cases where that can be very misleading. For some trees, it’s actually guaranteed to mislead you. So the latest work is on probabilistic methods that use this coalescent model to try and find the best species tree given a bunch of gene trees.
Going forward, what I’m interested in is ways of combining gene duplication and loss in deep coalescence. And for my work with gene duplication and loss, something that always – you know – if somebody wanted to give me a hard time during a talk. Like they say, well, what about deep coalescence? And I didn’t have a way to model it. And that’s cause there are no models that combine them both. So I’ve been working on a new model, DLCoal, that can do that. And I think it can do that pretty well. And what’s interesting is that there’s actually a third tree, besides just the gene tree and species tree, that allows you to model all these events. But there’s also work on modeling additional events, like horizontal transfer. So this is where two contemporary species exchange DNA. This is very frequent between prokaryotes. You basically can’t do phylogenetics without handling horizontal transfer in prokaryotes. So that is a very important problem for many people. And also, domain rearrangement – when a rearrangement occurs that moves a domain from one gene to another. So you can kind of mix and match genes. So the history of a gene changes across the tree.
